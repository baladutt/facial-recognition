{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "# load an image from file\n",
    "\n",
    "\n",
    "def vgg_16():\n",
    "    img = \"/Users/cgarg/Documents/charu/intuit/code/face/data/lfw/Ed_Wade/Ed_Wade_0001.jpg\"\n",
    "    image = load_img(img, target_size=(224, 224))\n",
    "    # convert the image pixels to a numpy array\n",
    "    image = img_to_array(image)\n",
    "    # reshape data for the model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # prepare the image for the VGG model\n",
    "    image = preprocess_input(image)\n",
    "    # load the model\n",
    "    vggModel = VGG16()\n",
    "    # predict the probability across all output classes\n",
    "    yhat = vggModel.predict(image)\n",
    "    # convert the probabilities to class labels\n",
    "    label = decode_predictions(yhat)\n",
    "    # retrieve the most likely result, e.g. highest probability\n",
    "    label = label[0][0]\n",
    "    # print the classification\n",
    "    print('%s (%.2f%%)' % (label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train, Test, Valid data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "# Fractions for each dataset\n",
    "train_frac = 0.70\n",
    "valid_frac = 0.05\n",
    "test_frac = 0.25\n",
    " \n",
    "# This function takes in np arrays of images and labels along with split fractions\n",
    "# and returns the six data arrays corresponding to each dataset as the appropriate type\n",
    "def create_data_splits(X, y, train_frac=train_frac, test_frac=test_frac, valid_frac=valid_frac):\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "#     print(\"x shape:: \", X.shape)\n",
    "#     print(\"y shape:: \", Y.shape)\n",
    "    labels = np.array([])\n",
    "    for index in range(len(y)):\n",
    "#         print(y[index][0])\n",
    "        labels = np.append(labels,inv_class_mapping[y[index][0]])\n",
    "    y = labels\n",
    " \n",
    " # Make sure that the fractions sum to 1.0\n",
    "    assert (test_frac + valid_frac + train_frac == 1.0), \"Test + Valid + Train Fractions must sum to 1.0\"\n",
    " \n",
    "    X_raw_test = []\n",
    "    X_raw_valid = []\n",
    "    X_raw_train = []\n",
    " \n",
    "    y_raw_test = []\n",
    "    y_raw_valid = []\n",
    "    y_raw_train = []\n",
    " \n",
    " # Randomly order the data and labels\n",
    "#     random_indices = np.random.permutation(len(X))\n",
    "    random_indices = np.random.permutation(len(X))\n",
    "#     print(random_indices)\n",
    "#     print(\"X len \", len(X))\n",
    "#     print(\"Y len \",len(Y))\n",
    "    X = X[random_indices]\n",
    "    y = y[random_indices]\n",
    "#     print(\"class images :\", class_images)\n",
    " \n",
    "    for image, label in zip(X, y):\n",
    "         \n",
    " # Number of images that correspond to desired fraction\n",
    "#         print(label , \":::\", class_images[label])\n",
    "        test_length = math.floor(test_frac * class_images[label])\n",
    "        valid_length = math.floor(valid_frac * class_images[label])\n",
    " \n",
    " # Check to see if the datasets have the right number of labels (and images)\n",
    "        if Counter(y_raw_test)[label] < test_length:\n",
    "            X_raw_test.append(image)\n",
    "            y_raw_test.append(label)\n",
    "        elif Counter(y_raw_valid)[label] < valid_length:\n",
    "            X_raw_valid.append(image)\n",
    "            y_raw_valid.append(label)\n",
    "        else:\n",
    "            X_raw_train.append(image)\n",
    "            y_raw_train.append(label)\n",
    "#     print(type(X_raw_train), \":: \", type(X_raw_valid), \": \", type(X_raw_test), \":: \", type(y_raw_train), \":: \", type(y_raw_valid), \":: \", type(y_raw_test))\n",
    "    np.array(X_raw_train)\n",
    "    return np.array(X_raw_train), np.array(X_raw_valid), np.array(X_raw_test), np.array(y_raw_train), np.array(y_raw_valid), np.array(y_raw_test)\n",
    "# Create all the testing splits using the create_splits function\n",
    "X_train, X_valid, X_test, y_train, y_valid, y_test = create_data_splits(X_transfer_learning, Y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.misc import imresize\n",
    "from PIL import Image\n",
    "# Function takes in an image array and returns the resized and normalized array\n",
    "def prepare_image(image, target_height=299, target_width=299):\n",
    "    if(type(image) == np.ndarray):\n",
    "        image = image[0].reshape(250,250,3)\n",
    "    else:   \n",
    "        image = image.iloc[0].reshape(250,250,3)\n",
    "    image = np.array(Image.fromarray(obj=image, mode='RGB').resize(size=(target_height,target_width), resample=Image.BICUBIC))\n",
    "    return image.astype(np.float32) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes in an array of images and labels and processes the images to create\n",
    "# a batch of a given size\n",
    "def create_batch(X, y, start_index=0, batch_size=4):\n",
    "#     print(\"create batch : start_index:: \", start_index, \"batch size:: \", batch_size)\n",
    "    stop_index = start_index + batch_size\n",
    "    prepared_images = []\n",
    "    labels = []\n",
    " \n",
    "    for index in range(start_index, stop_index):\n",
    "        if(type(X) == np.ndarray):\n",
    "            preparedImage = prepare_image(X[index]).reshape(299,299, 3)\n",
    "        else:    \n",
    "            preparedImage = prepare_image(X.iloc[index]).reshape(299,299, 3)\n",
    "        dim = np.zeros((299,299))\n",
    "        #preparedImage = np.stack((preparedImage,preparedImage, preparedImage), axis=-1)\n",
    "        prepared_images.append(preparedImage)\n",
    "        \n",
    "        if(type(y) == np.ndarray):\n",
    "            labels.append(int(y[index]))\n",
    "        else:    \n",
    "            labels.append(inv_class_mapping[y.iloc[index][0]])\n",
    " \n",
    " # Combine the images into a single array by joining along the 0th axis\n",
    "    X_batch = np.stack(prepared_images)\n",
    "    y_batch = np.array(labels, dtype=np.int32)\n",
    " \n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, y_valid = create_batch(X_transfer_learning, Y_sample, 0, len(X_transfer_learning))\n",
    "print(\"valid data shapes, X_valid: \",X_valid.shape, \", y_valid: \", y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup Inception ResNet V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of loading the inception v3 model\n",
    "from tensorflow.python.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "# load model\n",
    "model = InceptionResNetV2()\n",
    "# summarize the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "\n",
    "def predict_resnet_v2():\n",
    "    img = \"/Users/cgarg/Documents/charu/intuit/code/face/data/lfw/Elaine_Chao/Elaine_Chao_0001.jpg\"\n",
    "    image = load_img(img, target_size=(299, 299))\n",
    "    image = img_to_array(image)\n",
    "    # reshape data for the model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # prepare the image for the VGG model\n",
    "    image = preprocess_input(image)\n",
    "    ypred = model.predict(image)\n",
    "    # convert the probabilities to class labels\n",
    "    outLabel = decode_predictions(ypred)\n",
    "    # retrieve the most likely result, e.g. highest probability\n",
    "    outLabel = outLabel[0][0]\n",
    "    # print the classification\n",
    "    print('%s (%.2f%%)' % (outLabel[1], outLabel[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "from tensorflow.contrib.slim import nets\n",
    "from tensorflow.contrib.slim.nets import inception\n",
    "from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "from tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference\n",
    "# from sklearn.preprocessing import inception_preprocessing\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, [None, 299, 299, 3], name='X')\n",
    "is_training = tf.placeholder_with_default(False, [])\n",
    "# Run inception function to determine endpoints\n",
    "\n",
    "with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
    "     logits, end_points = inception.inception_v3(X, num_classes=1001, is_training=is_training)\n",
    "        \n",
    "inception_saver = tf.train.Saver()\n",
    "#print(end_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the trainable layer\n",
    "prelogits = tf.squeeze(end_points['PreLogits'], axis=[1,2])\n",
    "# Define the training layer and the new output layer\n",
    "n_outputs = len(class_mapping)\n",
    "with tf.name_scope(\"new_output_layer\"):\n",
    "    people_logits = tf.layers.dense(prelogits, n_outputs, name=\"people_logits\")\n",
    "    probability = tf.nn.softmax(people_logits, name='probability')\n",
    "# Placeholder for labels\n",
    "y = tf.placeholder(tf.int32, None)\n",
    "# Loss function and training operation\n",
    "# The training operation is passed the variables to train which includes only the single layer\n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=people_logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    " # Single layer to be trained\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"people_logits\")\n",
    " # The variables to train are passed to the training operation\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)\n",
    "\n",
    "    # Accuracy for network evaluation\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(predictions=people_logits, targets=y, k=1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    " # Intialization function and saver\n",
    "with tf.name_scope(\"init_and_saver\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "print(\"Done setup with n_outputs: \",n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trusty machine learning imports\n",
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# # Make sure to run notebook within slim folder\n",
    "# from datasets import dataset_utils\n",
    "# import os\n",
    "# # Base url\n",
    "# TF_MODELS_URL = \"http://download.tensorflow.org/models/\"\n",
    "# # Modify this path for a different CNN\n",
    "# INCEPTION_V3_URL = TF_MODELS_URL + \"inception_v3_2016_08_28.tar.gz\"\n",
    "# # Directory to save model checkpoints\n",
    "# MODELS_DIR = \"models/cnn\"\n",
    "# INCEPTION_V3_CKPT_PATH = MODELS_DIR + \"/inception_v3.ckpt\"\n",
    "# # Make the model directory if it does not exist\n",
    "# if not tf.gfile.Exists(MODELS_DIR):\n",
    "#     tf.gfile.MakeDirs(MODELS_DIR)\n",
    "# # Download the appropriate model if haven't already done so\n",
    "# if not os.path.exists(INCEPTION_V3_CKPT_PATH): \n",
    "#     dataset_utils.download_and_uncompress_tarball(INCEPTION_V3_URL, MODELS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"tensorboard_writing\"):\n",
    " # Track validation accuracy and loss and training accuracy\n",
    "    valid_acc_summary = tf.summary.scalar(name='valid_acc', tensor=accuracy)\n",
    "    valid_loss_summary = tf.summary.scalar(name='valid_loss', tensor=loss)\n",
    "    train_acc_summary = tf.summary.scalar(name='train_acc', tensor=accuracy)\n",
    "# Merge the validation stats\n",
    "    valid_merged_summary = tf.summary.merge(inputs=[valid_acc_summary, valid_loss_summary])\n",
    "# Use the time to differentiate the different training sessions\n",
    "from datetime import datetime\n",
    "import time\n",
    "# Specify the directory for the FileWriter\n",
    "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_dir = \"{}_unaugmented\".format(now)\n",
    "logdir = \"tensorboard/faces/\" + model_dir\n",
    "file_writer = tf.summary.FileWriter(logdir=logdir, graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# X_train = X_transfer_learning\n",
    "# X_valid = X_transfer_learning\n",
    "# y_train = Y_sample\n",
    "# y_valid = Y_sample\n",
    "n_epochs = 100\n",
    "batch_size = 32\n",
    "# Early stopping parameters\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.float(\"inf\")\n",
    "# Show progress every show_progress epochs\n",
    "show_progress = 1\n",
    "# Want to iterate through the entire training set every epoch\n",
    "n_iterations_per_epoch = len(X_train) // batch_size\n",
    "# Specify the directory for the FileWriter\n",
    "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_dir = \"{}_unaugmented\".format(now)\n",
    "logdir = \"tensorboard/faces/\" + model_dir\n",
    "file_writer = tf.summary.FileWriter(logdir=logdir, graph=tf.get_default_graph())\n",
    "# This is the pre-trained model checkpoint training path\n",
    "inception_v3_checkpoint_path = \"models/cnn/inception_v3.ckpt\"\n",
    "# This is the checkpoint path for our trained model with no dataaugmentation\n",
    "unaugmented_training_path = \"models/cnn/inception_v3_faces_unaugmented.ckpt\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    " # Restore all the weights from the original CNN\n",
    "    inception_saver.restore(sess, inception_v3_checkpoint_path)\n",
    " \n",
    "    t0 = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        start_index = 0\n",
    " # Each epoch, iterate through all the training instances\n",
    "        for iteration in range(n_iterations_per_epoch):\n",
    "             X_batch, y_batch = create_batch(X_train, y_train, start_index, batch_size)\n",
    " # Train the trainable layer \n",
    "             sess.run(training_op, {X: X_batch, y: y_batch})\n",
    "             start_index += batch_size\n",
    " \n",
    " # Display the progress of training and write to the TensorBoard directory\n",
    " # for later visualization of the training\n",
    "        if epoch % show_progress == 0:\n",
    "            train_summary = sess.run(train_acc_summary, {X: X_batch, y: y_batch})\n",
    "            file_writer.add_summary(train_summary, (epoch+1))\n",
    " #Size for validation limited by GPU memory (68 images will work)\n",
    "            valid_loss, valid_acc, valid_summary = sess.run([loss, accuracy, valid_merged_summary], {X: X_valid, y: y_valid})\n",
    "            file_writer.add_summary(valid_summary, (epoch+1))\n",
    "            print('Epoch: {:4} Validation Loss: {:.4f} Accuracy: {:4f}'.format(epoch+1, valid_loss, valid_acc))\n",
    " \n",
    " # Check to see if network is still improving, if improved during epoch\n",
    " # a snapshot of the model will be saved to retain the best model\n",
    "            if valid_loss < best_loss:\n",
    "                best_loss = valid_loss\n",
    "                checks_without_progess = 0\n",
    "                save_path = saver.save(sess, unaugmented_training_path)\n",
    "\n",
    " # If network is not improving for a specified number of epochs, stop training\n",
    "            else:\n",
    "                checks_without_progress += 1\n",
    "                if checks_without_progress > max_checks_without_progress:\n",
    "                    print('Stopping Early! Loss has not improved in {} epochs'.format(max_checks_without_progress))\n",
    "                    break\n",
    " \n",
    "    t1 = time.time()\n",
    " \n",
    "    print('Total Training Time: {:.2f} minutes'.format( (t1-t0) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compute Test Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = 32\n",
    "# X_test = X_transfer_learning\n",
    "# y_test = Y_sample\n",
    "n_iterations = len(X_test)//eval_batch_size\n",
    "with tf.Session() as sess:\n",
    " # Restore the new trained model \n",
    " saver.restore(sess, unaugmented_training_path)\n",
    " \n",
    " start_index = 0\n",
    " # Create a dictionary to store all the accuracies\n",
    " test_acc = {}\n",
    " \n",
    " t0 = time.time()\n",
    " # Iterate through entire testing set one batch at a time\n",
    " for iteration in range(n_iterations):\n",
    "    X_test_batch, y_test_batch = create_batch(X_test, y_test, start_index, batch_size=eval_batch_size)\n",
    "    test_acc[iteration] = accuracy.eval({X: X_test_batch, y:y_test_batch})\n",
    "    start_index += eval_batch_size\n",
    "    print('Iteration: {} Batch Testing Accuracy: {:.2f}%'.format(\n",
    "    iteration+1, test_acc[iteration] * 100))\n",
    " \n",
    "    t1 = time.time()\n",
    " \n",
    " # Final accuracy is mean of each batch accuracy\n",
    "print('\\nFinal Testing Accuracy: {:.4f}% on {} instances.'.format(\n",
    "np.mean(list(test_acc.values())) * 100, len(X_test)))\n",
    "print('Total evaluation time: {:.4f} seconds'.format((t1-t0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
